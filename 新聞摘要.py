#%%
import json
import os
import streamlit as st
from snownlp import SnowNLP
import jieba.analyse
import datetime

#%%
st.set_page_config(

    layout="wide"

)
#%%
def normalize_text(text):
    """çµ±ä¸€å°/è‡ºå­—å…ƒ"""
    return text.replace('è‡º', 'å°')

#%%
st.title('æ–°èè³‡æ–™åº«')
st.markdown("----")
c1, c2 = st.columns(2)
with c1:
    date_filter_s = st.date_input("ç¯©é¸æ–°èæ—¥æœŸï¼ˆé–‹å§‹æ™‚é–“ï¼‰", value=datetime.date(2025, 6, 28), min_value=datetime.date(2025, 6, 28))
with c2:
    date_filter_e = st.date_input("ç¯©é¸æ–°èæ—¥æœŸï¼ˆçµæŸæ™‚é–“ï¼‰", value=datetime.date.today(), min_value=datetime.date(2025, 6, 28))
st.markdown("----")

# å‹•æ…‹æ–°å¢é—œéµè©è¼¸å…¥æ¡†
if 'keyword_count' not in st.session_state:
    st.session_state.keyword_count = 0

col1, col2 = st.columns(2)
with col1:
    st.write("**æ–°èé—œéµè©æœå°‹:**")
    if st.button("æ–°å¢é—œéµè©"):
        st.session_state.keyword_count += 1

with col2:
    st.write("**æ¸›å°‘é—œéµè©æœå°‹:**")
    if st.button("æ¸›å°‘é—œéµè©"):
        if st.session_state.keyword_count > 0:
            st.session_state.keyword_count -= 1

# æ”¶é›†æ‰€æœ‰é—œéµè©
keywords = []
for i in range(st.session_state.keyword_count):
    keyword = st.text_input(f'é—œéµè© {i+1}', key=f"news_keyword{i+1}")
    if keyword.strip():  # åªæ”¶é›†éç©ºçš„é—œéµè©
        keywords.append(keyword.strip())

# é¡¯ç¤ºæœå°‹ç‹€æ…‹
if keywords:
    st.info(f"å°‡æœå°‹åŒ…å«ä»¥ä¸‹é—œéµè©çš„æ–°è: {', '.join(keywords)}")
else:
    st.info("æœªè¼¸å…¥é—œéµè©ï¼Œå°‡é¡¯ç¤ºæ‰€æœ‰æ–°è")

st.markdown("----")


######################################################################################
#%%
def keyword_summary(text, num_sentences=3):
    """åŸºæ–¼é—œéµè©çš„æ‘˜è¦"""
    keywords = jieba.analyse.extract_tags(text, topK=10)
    sentences = text.split('ã€‚')
    sentences = [s.strip() for s in sentences if s.strip()]
    
    sentence_scores = []
    for sentence in sentences:
        score = sum(1 for kw in keywords if kw in sentence)
        sentence_scores.append((sentence, score))
    
    top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)
    return [s[0] for s in top_sentences[:num_sentences] if s[0]]



#%%
json_folder = os.path.join(os.path.dirname(__file__), 'data')  # ç›¸å°æ–¼ç¨‹å¼ç¢¼æª”æ¡ˆçš„ data è³‡æ–™å¤¾
news_path = os.path.join(json_folder, 'combined_news.json')
# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
if not os.path.exists(json_folder):
    st.error(f"æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {json_folder}")
    st.stop()

# ç¢ºä¿ JSON æª”æ¡ˆå­˜åœ¨
if not os.path.exists(news_path):
    st.error(f"æ‰¾ä¸åˆ°æ–°èæª”æ¡ˆ: {news_path}")
    st.stop()
    
with open(news_path, 'r', encoding='utf-8') as f:
    news_list = json.load(f)

# ç¯©é¸é‚è¼¯
filtered_news = []

for news in news_list:
    # 1. æ—¥æœŸç¯©é¸
    news_date = datetime.datetime.strptime(news['æ—¥æœŸ'], '%Y-%m-%d').date()
    if not (date_filter_s <= news_date <= date_filter_e):
        continue
    
    # 2. é—œéµè©ç¯©é¸
    if keywords:
        # å°‡æ¨™é¡Œå’Œå…§å®¹çµ±ä¸€å­—å…ƒ
        text_to_search = normalize_text(news['æ¨™é¡Œ'] + news['å…§å®¹'])
        # å°‡é—œéµè©ä¹Ÿçµ±ä¸€å­—å…ƒ
        normalized_keywords = [normalize_text(keyword) for keyword in keywords]
        
        if not all(keyword in text_to_search for keyword in normalized_keywords):
            continue
    
    # é€šéç¯©é¸çš„æ–°èåŠ å…¥åˆ—è¡¨
    filtered_news.append(news)

# é¡¯ç¤ºç¯©é¸çµæœ
st.write(f"**ç¯©é¸çµæœ: å…± {len(filtered_news)} å‰‡æ–°è**")
st.markdown("----")

# æ–°å¢åŸ·è¡Œæ‘˜è¦çš„æŒ‰éˆ•
if st.button("ğŸ” é–‹å§‹é€²è¡Œæ–°èæ‘˜è¦", type="primary"):
    if filtered_news:
        # é¡¯ç¤ºé€²åº¦æ¢
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        #%%
        # å°ç¯©é¸å¾Œçš„æ–°èé€²è¡Œæ‘˜è¦
        for idx, news in enumerate(filtered_news):
            # æ›´æ–°é€²åº¦
            progress = (idx + 1) / len(filtered_news)
            progress_bar.progress(progress)
            status_text.text(f'æ­£åœ¨è™•ç†ç¬¬ {idx + 1}/{len(filtered_news)} å‰‡æ–°è...')
            
            text = news['å…§å®¹']
            
            # æ–¹æ³•1: SnowNLP TextRank
            s = SnowNLP(text)
            textrank_summary = list(s.summary(3))
            
            # æ–¹æ³•2: é—œéµè©æ‘˜è¦
            keyword_sum = keyword_summary(text, 3)
            
            st.subheader(f"{news['æ¨™é¡Œ']}", divider='rainbow')
            st.write(f"æ—¥æœŸ: {news['æ—¥æœŸ']}")
            
            with st.expander("æŸ¥çœ‹å®Œæ•´æ–°èå…§å®¹"):
                st.code(news['å…§å®¹'])

            st.write("**TextRank æ‘˜è¦:**")
            for i, sentence in enumerate(textrank_summary, 1):
                st.write(f"{i}. {sentence}")
            
            st.write("**é—œéµè©æ‘˜è¦:**")
            for i, sentence in enumerate(keyword_sum, 1):
                st.write(f"{i}. {sentence}")
            
            # ä¿å­˜å…©ç¨®æ‘˜è¦
            news['TextRankæ‘˜è¦'] = textrank_summary
            news['é—œéµè©æ‘˜è¦'] = keyword_sum
            
            if 'å…§å®¹' in news:
                del news['å…§å®¹']
        
        # æ¸…é™¤é€²åº¦æ¢
        progress_bar.empty()
        status_text.empty()
    else:
        st.warning("æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„æ–°èï¼Œç„¡æ³•é€²è¡Œæ‘˜è¦")
else:
    if filtered_news:
        st.info("ğŸ“‹ é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹é€²è¡Œæ–°èæ‘˜è¦")
    else:
        st.warning("æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„æ–°è")